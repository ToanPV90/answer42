# 9.6.2 Concrete Agent Implementations with AIConfig Integration

## Overview

This document provides complete implementations of specialized agents that inherit from the base `AbstractConfigurableAgent` class and integrate with Answer42's existing `AIConfig` and `ThreadConfig` infrastructure.

## Core Processing Agents

### Paper Processor Agent

**Responsibility**: Text extraction and document parsing using OpenAI via AIConfig
**Provider**: OpenAI GPT-4 (via `AIConfig.openAiChatClient()`)
**ThreadConfig Integration**: Async processing using `taskExecutor`

```java
@Component
public class PaperProcessorAgent extends OpenAIBasedAgent {
    private final PDFTextExtractor pdfExtractor;
    private final StructureAnalyzer structureAnalyzer;
    private final DocumentSectionizer sectionizer;
    
    public PaperProcessorAgent(
            AIConfig aiConfig, 
            ThreadConfig threadConfig,
            PDFTextExtractor pdfExtractor,
            StructureAnalyzer structureAnalyzer,
            DocumentSectionizer sectionizer) {
        super(aiConfig, threadConfig);
        this.pdfExtractor = pdfExtractor;
        this.structureAnalyzer = structureAnalyzer;
        this.sectionizer = sectionizer;
    }
    
    @Override
    public AgentType getAgentType() {
        return AgentType.PAPER_PROCESSOR;
    }
    
    @Override
    protected AgentResult processWithConfig(AgentTask task) {
        LoggingUtil.info(LOG, "processWithConfig", "Processing paper %s", task.getPaperId());
        
        try {
            PaperProcessingRequest request = task.getPaperProcessingRequest();
            
            // Step 1: Extract raw text from PDF using ThreadConfig executor
            CompletableFuture<RawTextResult> textFuture = CompletableFuture.supplyAsync(
                () -> pdfExtractor.extractText(request.getInputFile()), taskExecutor);
            
            // Step 2: Parse document structure in parallel
            CompletableFuture<StructureAnalysis> structureFuture = textFuture.thenApplyAsync(
                rawText -> analyzeStructureWithAI(rawText, request), taskExecutor);
            
            // Step 3: Sectionize document
            CompletableFuture<SectionizedDocument> sectionFuture = structureFuture.thenApplyAsync(
                structure -> sectionizeDocument(structure, request), taskExecutor);
            
            // Wait for all processing to complete
            SectionizedDocument document = sectionFuture.get();
            
            return AgentResult.success(document, createProcessingMetrics());
            
        } catch (Exception e) {
            LoggingUtil.error(LOG, "processWithConfig", "Failed to process paper", e);
            return AgentResult.failure(e);
        }
    }
    
    private StructureAnalysis analyzeStructureWithAI(RawTextResult rawText, PaperProcessingRequest request) {
        Prompt structurePrompt = optimizePromptForOpenAI("""
            Analyze the following academic paper text and identify its structure:
            
            Text: {text}
            
            Please identify and extract:
            1. Title and authors with affiliations
            2. Abstract (complete text)
            3. Main sections with boundaries:
               - Introduction
               - Methods/Methodology
               - Results
               - Discussion
               - Conclusion
               - References
            4. Tables and figures with captions
            5. Mathematical equations and formulas
            6. In-text citations and their contexts
            
            Return as structured JSON with precise character positions for each section.
            Include confidence scores for each identified element.
            """, Map.of(
                "text", rawText.getContent()
            ));
        
        ChatResponse response = executePrompt(structurePrompt);
        return parseStructureResponse(response.getResult().getOutput().getContent());
    }
    
    private SectionizedDocument sectionizeDocument(StructureAnalysis structure, PaperProcessingRequest request) {
        return sectionizer.createSectionizedDocument(structure, request.getProcessingOptions());
    }
    
    @Override
    public Duration estimateProcessingTime(AgentTask task) {
        PaperProcessingRequest request = task.getPaperProcessingRequest();
        
        // Estimate based on document size and complexity
        long fileSize = request.getInputFile().length();
        int pageCount = request.getEstimatedPageCount();
        
        // Base time: 30 seconds + 5 seconds per page + file size factor
        long baseSeconds = 30;
        long pageSeconds = pageCount * 5;
        long sizeSeconds = Math.min(fileSize / 100000, 60); // Max 60 seconds for size
        
        return Duration.ofSeconds(baseSeconds + pageSeconds + sizeSeconds);
    }
}
```

### Metadata Enhancement Agent

**Responsibility**: External metadata verification using OpenAI via AIConfig
**Provider**: OpenAI GPT-4 (via `AIConfig.openAiChatClient()`)
**ThreadConfig Integration**: Parallel API calls using `taskExecutor`

```java
@Component
public class MetadataEnhancementAgent extends OpenAIBasedAgent {
    private final CrossrefApiClient crossrefClient;
    private final SemanticScholarApiClient semanticScholarClient;
    private final DOIResolver doiResolver;
    private final AuthorDisambiguationService authorService;
    
    public MetadataEnhancementAgent(
            AIConfig aiConfig,
            ThreadConfig threadConfig,
            CrossrefApiClient crossrefClient,
            SemanticScholarApiClient semanticScholarClient,
            DOIResolver doiResolver,
            AuthorDisambiguationService authorService) {
        super(aiConfig, threadConfig);
        this.crossrefClient = crossrefClient;
        this.semanticScholarClient = semanticScholarClient;
        this.doiResolver = doiResolver;
        this.authorService = authorService;
    }
    
    @Override
    public AgentType getAgentType() {
        return AgentType.METADATA_ENHANCER;
    }
    
    @Override
    protected AgentResult processWithConfig(AgentTask task) {
        Paper paper = task.getPaper();
        MetadataEnhancementRequest request = task.getMetadataRequest();
        
        try {
            // Execute multiple metadata enhancement sources in parallel using ThreadConfig executor
            List<CompletableFuture<MetadataSource>> enhancementFutures = List.of(
                CompletableFuture.supplyAsync(() -> enhanceWithCrossref(paper), taskExecutor),
                CompletableFuture.supplyAsync(() -> enhanceWithSemanticScholar(paper), taskExecutor),
                CompletableFuture.supplyAsync(() -> enhanceWithDOIResolution(paper), taskExecutor),
                CompletableFuture.supplyAsync(() -> enhanceWithAuthorDisambiguation(paper), taskExecutor)
            );
            
            // Wait for all enhancements to complete
            CompletableFuture.allOf(enhancementFutures.toArray(new CompletableFuture[0])).join();
            
            // Collect results
            List<MetadataSource> sources = enhancementFutures.stream()
                .map(CompletableFuture::join)
                .filter(Objects::nonNull)
                .collect(Collectors.toList());
            
            // Synthesize results using AIConfig chat client
            EnhancedMetadata enhanced = synthesizeMetadataWithAI(paper, sources);
            
            return AgentResult.success(enhanced, createProcessingMetrics());
            
        } catch (Exception e) {
            LoggingUtil.error(LOG, "processWithConfig", "Metadata enhancement failed", e);
            return AgentResult.failure(e);
        }
    }
    
    private MetadataSource enhanceWithCrossref(Paper paper) {
        try {
            if (paper.getDoi() != null) {
                CrossrefMetadata metadata = crossrefClient.getMetadataByDOI(paper.getDoi());
                return new MetadataSource("crossref", metadata, 0.9);
            } else if (paper.getTitle() != null) {
                List<CrossrefMetadata> candidates = crossrefClient.searchByTitle(paper.getTitle());
                return new MetadataSource("crossref", selectBestMatch(candidates, paper), 0.7);
            }
        } catch (Exception e) {
            LoggingUtil.warn(LOG, "enhanceWithCrossref", "Crossref enhancement failed", e);
        }
        return null;
    }
    
    private MetadataSource enhanceWithSemanticScholar(Paper paper) {
        try {
            SemanticScholarMetadata metadata;
            if (paper.getDoi() != null) {
                metadata = semanticScholarClient.getByDOI(paper.getDoi());
            } else {
                metadata = semanticScholarClient.searchByTitle(paper.getTitle());
            }
            return new MetadataSource("semantic_scholar", metadata, 0.85);
        } catch (Exception e) {
            LoggingUtil.warn(LOG, "enhanceWithSemanticScholar", "Semantic Scholar enhancement failed", e);
        }
        return null;
    }
    
    private MetadataSource enhanceWithDOIResolution(Paper paper) {
        try {
            if (paper.getDoi() != null) {
                ResolvedDOI resolved = doiResolver.resolve(paper.getDoi());
                return new MetadataSource("doi_resolution", resolved, 0.95);
            }
        } catch (Exception e) {
            LoggingUtil.warn(LOG, "enhanceWithDOIResolution", "DOI resolution failed", e);
        }
        return null;
    }
    
    private MetadataSource enhanceWithAuthorDisambiguation(Paper paper) {
        try {
            List<DisambiguatedAuthor> authors = authorService.disambiguateAuthors(paper.getAuthors());
            return new MetadataSource("author_disambiguation", authors, 0.8);
        } catch (Exception e) {
            LoggingUtil.warn(LOG, "enhanceWithAuthorDisambiguation", "Author disambiguation failed", e);
        }
        return null;
    }
    
    private EnhancedMetadata synthesizeMetadataWithAI(Paper paper, List<MetadataSource> sources) {
        Prompt synthesisPrompt = optimizePromptForOpenAI("""
            Synthesize the following metadata from multiple sources into a coherent, authoritative record.
            Resolve conflicts by prioritizing sources based on their confidence scores and reliability.
            
            Original Paper: {paper}
            
            Metadata Sources:
            {sources}
            
            For each field, determine:
            1. The most authoritative value
            2. Confidence level (0-1)
            3. Source of the information
            4. Any conflicts or inconsistencies
            
            Return structured JSON with:
            - Synthesized metadata fields
            - Confidence scores for each field
            - Source attribution
            - Conflict resolution notes
            """, Map.of(
                "paper", paper.toMetadataString(),
                "sources", sources.stream()
                    .map(MetadataSource::toDetailedString)
                    .collect(Collectors.joining("\n\n"))
            ));
        
        ChatResponse response = executePrompt(synthesisPrompt);
        return parseEnhancedMetadata(response.getResult().getOutput().getContent());
    }
    
    @Override
    public Duration estimateProcessingTime(AgentTask task) {
        // Metadata enhancement depends on external API response times
        // Estimate: 30 seconds base + 15 seconds per API source
        return Duration.ofSeconds(30 + (4 * 15)); // 4 sources = 90 seconds total
    }
}
```

## Analysis Agents

### Content Summarizer Agent

**Responsibility**: Multi-level summary generation using Anthropic Claude via AIConfig
**Provider**: Anthropic Claude (via `AIConfig.anthropicChatClient()`)
**ThreadConfig Integration**: Parallel summary generation using `taskExecutor`

```java
@Component
public class ContentSummarizerAgent extends AnthropicBasedAgent {
    private final SummaryTemplateService templateService;
    private final SummaryLevelConfig summaryConfig;
    private final ContentPreprocessor preprocessor;
    
    public ContentSummarizerAgent(
            AIConfig aiConfig,
            ThreadConfig threadConfig,
            SummaryTemplateService templateService,
            SummaryLevelConfig summaryConfig,
            ContentPreprocessor preprocessor) {
        super(aiConfig, threadConfig);
        this.templateService = templateService;
        this.summaryConfig = summaryConfig;
        this.preprocessor = preprocessor;
    }
    
    @Override
    public AgentType getAgentType() {
        return AgentType.CONTENT_SUMMARIZER;
    }
    
    @Override
    protected AgentResult processWithConfig(AgentTask task) {
        StructuredDocument document = task.getDocument();
        SummaryRequest request = task.getSummaryRequest();
        
        try {
            // Preprocess content for summarization
            ProcessedContent processedContent = preprocessor.prepare(document, request);
            
            // Generate summaries for each level in parallel using ThreadConfig executor
            Map<SummaryLevel, CompletableFuture<SummaryEntry>> summaryFutures = 
                request.getRequestedLevels().stream()
                    .collect(Collectors.toMap(
                        level -> level,
                        level -> CompletableFuture.supplyAsync(
                            () -> generateSummaryForLevel(processedContent, level, request), 
                            taskExecutor)
                    ));
            
            // Wait for all summaries to complete
            CompletableFuture.allOf(summaryFutures.values().toArray(new CompletableFuture[0])).join();
            
            // Collect results
            Map<SummaryLevel, SummaryEntry> summaries = summaryFutures.entrySet().stream()
                .collect(Collectors.toMap(
                    Map.Entry::getKey,
                    entry -> entry.getValue().join()
                ));
            
            // Generate cross-summary analysis
            SummaryAnalysis analysis = generateSummaryAnalysis(summaries, document);
            
            return AgentResult.success(
                new SummaryResult(summaries, analysis), 
                createProcessingMetrics()
            );
            
        } catch (Exception e) {
            LoggingUtil.error(LOG, "processWithConfig", "Summary generation failed", e);
            return AgentResult.failure(e);
        }
    }
    
    private SummaryEntry generateSummaryForLevel(
            ProcessedContent content, 
            SummaryLevel level, 
            SummaryRequest request) {
        
        SummaryConfiguration config = summaryConfig.getConfig(level);
        String template = templateService.getTemplate(level, request.getStyle());
        
        Prompt summaryPrompt = optimizePromptForAnthropic(template, Map.of(
            "title", content.getTitle(),
            "abstract", content.getAbstract(),
            "introduction", content.getIntroduction(),
            "methods", content.getMethods(),
            "results", content.getResults(),
            "discussion", content.getDiscussion(),
            "conclusion", content.getConclusion(),
            "maxWords", config.getMaxWords(),
            "style", config.getStyle(),
            "audience", config.getTargetAudience(),
            "focusAreas", request.getFocusAreas(),
            "includeElements", config.getRequiredElements()
        ));
        
        ChatResponse response = executePrompt(summaryPrompt);
        String summaryContent = response.getResult().getOutput().getContent();
        
        // Analyze summary quality
        SummaryQuality quality = analyzeSummaryQuality(summaryContent, config, content);
        
        return new SummaryEntry(
            level,
            summaryContent,
            quality,
            response.getMetadata().getUsage(),
            System.currentTimeMillis()
        );
    }
    
    private SummaryAnalysis generateSummaryAnalysis(
            Map<SummaryLevel, SummaryEntry> summaries, 
            StructuredDocument document) {
        
        // Analyze consistency across summary levels
        ConsistencyAnalysis consistency = analyzeConsistency(summaries);
        
        // Identify key themes across all summaries
        List<String> keyThemes = extractKeyThemes(summaries, document);
        
        // Generate improvement recommendations
        List<String> recommendations = generateRecommendations(summaries, consistency);
        
        return new SummaryAnalysis(consistency, keyThemes, recommendations);
    }
    
    private SummaryQuality analyzeSummaryQuality(
            String summary, 
            SummaryConfiguration config, 
            ProcessedContent content) {
        
        // Word count analysis
        int actualWords = summary.split("\\s+").length;
        boolean withinWordLimit = actualWords <= config.getMaxWords();
        
        // Content coverage analysis (simplified)
        boolean coversKeyPoints = summary.toLowerCase().contains("method") && 
                                 summary.toLowerCase().contains("result");
        
        // Readability score (simplified)
        double readabilityScore = calculateReadabilityScore(summary);
        
        return new SummaryQuality(
            withinWordLimit,
            coversKeyPoints,
            readabilityScore,
            actualWords,
            config.getMaxWords()
        );
    }
    
    @Override
    public Duration estimateProcessingTime(AgentTask task) {
        SummaryRequest request = task.getSummaryRequest();
        StructuredDocument document = task.getDocument();
        
        // Estimate based on content length and number of summary levels
        int contentLength = document.getMainContent().length();
        int summaryLevels = request.getRequestedLevels().size();
        
        // Base: 60 seconds + 30 seconds per level + content factor
        long baseSeconds = 60;
        long levelSeconds = summaryLevels * 30;
        long contentSeconds = Math.min(contentLength / 5000, 120); // Max 2 minutes for content
        
        return Duration.ofSeconds(baseSeconds + levelSeconds + contentSeconds);
    }
}
```

### Concept Explainer Agent

**Responsibility**: Technical term explanation using OpenAI via AIConfig
**Provider**: OpenAI GPT-4 (via `AIConfig.openAiChatClient()`)
**ThreadConfig Integration**: Parallel explanation generation using `taskExecutor`

```java
@Component
public class ConceptExplainerAgent extends OpenAIBasedAgent {
    private final ConceptDatabase conceptDb;
    private final EducationLevelMapper levelMapper;
    private final TechnicalTermExtractor termExtractor;
    private final AnalgyGenerator analogyGenerator;
    
    public ConceptExplainerAgent(
            AIConfig aiConfig,
            ThreadConfig threadConfig,
            ConceptDatabase conceptDb,
            EducationLevelMapper levelMapper,
            TechnicalTermExtractor termExtractor,
            AnalgyGenerator analogyGenerator) {
        super(aiConfig, threadConfig);
        this.conceptDb = conceptDb;
        this.levelMapper = levelMapper;
        this.termExtractor = termExtractor;
        this.analogyGenerator = analogyGenerator;
    }
    
    @Override
    public AgentType getAgentType() {
        return AgentType.CONCEPT_EXPLAINER;
    }
    
    @Override
    protected AgentResult processWithConfig(AgentTask task) {
        StructuredDocument document = task.getDocument();
        ExplanationRequest request = task.getExplanationRequest();
        
        try {
            // Extract technical terms from document
            Set<String> technicalTerms = termExtractor.extractTerms(document, request.getDomain());
            
            // Prioritize terms based on importance and complexity
            List<String> prioritizedTerms = prioritizeTerms(technicalTerms, document, request);
            
            // Generate explanations for each education level in parallel
            Map<EducationLevel, CompletableFuture<LevelExplanations>> explanationFutures = 
                request.getTargetLevels().stream()
                    .collect(Collectors.toMap(
                        level -> level,
                        level -> CompletableFuture.supplyAsync(
                            () -> generateExplanationsForLevel(prioritizedTerms, document, level), 
                            taskExecutor)
                    ));
            
            // Wait for all explanations to complete
            CompletableFuture.allOf(explanationFutures.values().toArray(new CompletableFuture[0])).join();
            
            // Collect results
            Map<EducationLevel, LevelExplanations> explanations = explanationFutures.entrySet().stream()
                .collect(Collectors.toMap(
                    Map.Entry::getKey,
                    entry -> entry.getValue().join()
                ));
            
            // Generate concept relationship map
            ConceptRelationshipMap relationshipMap = generateRelationshipMap(prioritizedTerms, document);
            
            return AgentResult.success(
                new ConceptExplanationResult(explanations, relationshipMap),
                createProcessingMetrics()
            );
            
        } catch (Exception e) {
            LoggingUtil.error(LOG, "processWithConfig", "Concept explanation failed", e);
            return AgentResult.failure(e);
        }
    }
    
    private List<String> prioritizeTerms(
            Set<String> terms, 
            StructuredDocument document, 
            ExplanationRequest request) {
        
        return terms.stream()
            .map(term -> new TermImportance(term, calculateImportance(term, document)))
            .sorted(Comparator.comparingDouble(TermImportance::getImportance).reversed())
            .limit(request.getMaxTerms())
            .map(TermImportance::getTerm)
            .collect(Collectors.toList());
    }
    
    private LevelExplanations generateExplanationsForLevel(
            List<String> terms, 
            StructuredDocument document, 
            EducationLevel level) {
        
        Map<String, ConceptExplanation> explanations = new HashMap<>();
        
        // Process terms in batches to optimize API usage
        List<List<String>> termBatches = partitionTerms(terms, 5); // 5 terms per batch
        
        for (List<String> batch : termBatches) {
            Map<String, ConceptExplanation> batchExplanations = 
                generateBatchExplanations(batch, document, level);
            explanations.putAll(batchExplanations);
        }
        
        return new LevelExplanations(level, explanations);
    }
    
    private Map<String, ConceptExplanation> generateBatchExplanations(
            List<String> terms, 
            StructuredDocument document, 
            EducationLevel level) {
        
        Prompt explanationPrompt = optimizePromptForOpenAI("""
            Explain the following technical concepts from this academic paper 
            for a {level} education level audience:
            
            Paper Title: {title}
            Paper Context: {context}
            Research Domain: {domain}
            
            Terms to Explain: {terms}
            
            For each term, provide:
            1. Simple definition appropriate for {level} level
            2. Real-world analogy if helpful (especially for complex concepts)
            3. Why it's important in this specific research
            4. Related concepts the reader should understand
            5. Common misconceptions to avoid
            6. Prerequisite knowledge needed
            
            Use clear, engaging language that respects the reader's intelligence
            while making complex concepts accessible.
            
            Return as structured JSON with each term as a key.
            """, Map.of(
                "level", level.getDisplayName(),
                "title", document.getTitle(),
                "context", document.getAbstractAndIntroduction(),
                "domain", document.getResearchDomain(),
                "terms", String.join(", ", terms)
            ));
        
        ChatResponse response = executePrompt(explanationPrompt);
        return parseExplanationResponse(response.getResult().getOutput().getContent());
    }
    
    private ConceptRelationshipMap generateRelationshipMap(
            List<String> terms, 
            StructuredDocument document) {
        
        Prompt relationshipPrompt = optimizePromptForOpenAI("""
            Analyze the relationships between these technical concepts 
            in the context of this research paper:
            
            Paper: {title}
            Concepts: {terms}
            
            Identify:
            1. Hierarchical relationships (parent-child concepts)
            2. Causal relationships (A leads to B)
            3. Dependency relationships (A requires understanding of B)
            4. Similarity relationships (A is similar to B)
            5. Opposition relationships (A vs B)
            
            Create a concept map showing these relationships.
            Return as structured JSON with nodes and edges.
            """, Map.of(
                "title", document.getTitle(),
                "terms", String.join(", ", terms)
            ));
        
        ChatResponse response = executePrompt(relationshipPrompt);
        return parseRelationshipMap(response.getResult().getOutput().getContent());
    }
    
    @Override
    public Duration estimateProcessingTime(AgentTask task) {
        ExplanationRequest request = task.getExplanationRequest();
        
        // Estimate based on number of terms and education levels
        int termCount = Math.min(request.getMaxTerms(), 20); // Cap at 20 terms
        int levelCount = request.getTargetLevels().size();
        
        // Base: 45 seconds + 10 seconds per term per level
        long baseSeconds = 45;
        long processingSeconds = termCount * levelCount * 10;
        
        return Duration.ofSeconds(baseSeconds + processingSeconds);
    }
}
```

This implementation shows how concrete agents:

1. **Inherit from provider-specific base classes** (OpenAIBasedAgent, AnthropicBasedAgent)
2. **Use AIConfig's chat clients** with user-specific API keys
3. **Leverage ThreadConfig's task executor** for parallel processing
4. **Implement proper error handling** and logging
5. **Provide realistic time estimates** based on task complexity
6. **Generate comprehensive processing metrics** for monitoring

Each agent is optimized for its specific AI provider while maintaining consistent behavior patterns through the base implementation.
